extra_num_layers: 0
extra_hidden_size: 128
embed_size: 64

transformer_input_size: null
transformer_num_layers: 4
transformer_num_heads: 6
transformer_head_output_size: 64
transformer_mlp_hidden_size: 256
transformer_dropout: 0.0
transformer_max_seq_len: 10

# Added arguments
# device: "cuda"
use_gripper: true
use_joint: true
use_ee: false
use_position_encoding: true

defaults:
    - position_encoding@temporal_position_encoding: sinusoidal_position_encoding.yaml
    - value_head: factored_head.yaml